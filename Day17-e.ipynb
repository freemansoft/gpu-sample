{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69da099-79dd-4e05-9dd0-b1e5d798b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Attempt at a modified version D onto a GPU.\n",
    "#\n",
    "# This won't work because all the threads in a warp must execute the exact same \n",
    "# instructions which may not happen here because of branching and jump behavior\n",
    "#\n",
    "import os\n",
    "# needs to appear before CUDA import\n",
    "os.environ[\"NUMBA_ENABLE_CUDASIM\"] = \"0\"\n",
    "# set to \"1\" for more debugging, but slower performance\n",
    "os.environ[\"NUMBA_CUDA_DEBUGINFO\"] = \"0\"\n",
    "\n",
    "from numba import jit, njit, cuda , prange\n",
    "import numba\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# original problem\n",
    "# registers = [729, 0, 0]\n",
    "# program = [0,1,5,4,3,0]\n",
    "\n",
    "# enabling the JIT may cause print statements not to print lists as expected\n",
    "# Return array passed in as a parameter because GPU can only modify passed-in arrays can't return anything\n",
    "# This kernel function is invoked once per thread\n",
    "@cuda.jit()\n",
    "def loop_em(program, num_match_elements, return_array, start, end):\n",
    "    \n",
    "    register_a = 0\n",
    "    register_b = 0\n",
    "    register_c = 0\n",
    "\n",
    "    # cuda.threadIdx.x should range [0..cuda.blockDim.x] [0..256]\n",
    "    # cuda.blockIdx.x should range  [0..cuda.gridDim.x] [0..(4*38)]\n",
    "    # cuda unique thread identifier\n",
    "    global_thread_id = cuda.grid(1)\n",
    "    # print(cuda.blockDim.x * 10000000000 + cuda.blockIdx.x * 1000000 + cuda.threadIdx.x )\n",
    "\n",
    "    # handle if not exactly divisible\n",
    "    # how many numbers we sweep in each thread\n",
    "    cycles_per_thread = ((end - start) // cuda.gridsize(1))+1\n",
    "\n",
    "    # This has to be a constant due to CUDA rules so we pick something longer than our test program\n",
    "    # Create an array, and assume it is no longer than our expected program.  This is a testing shortcut\n",
    "    output = cuda.local.array(20, np.int64) # output array\n",
    "\n",
    "    for cycle_index in range(cycles_per_thread):\n",
    "        # which thread block are we in?\n",
    "        global_cycle_id = cycle_index + (global_thread_id * cycles_per_thread)\n",
    "        # handle the fact the batch size is not an exact factor of the total range\n",
    "        if (global_cycle_id <= end):\n",
    "            # This program is 8 octal operands so the A register needs to be 8 octal digits.\n",
    "            # Registers are reset at the start of every run\n",
    "            register_a = global_cycle_id\n",
    "            register_b = 0\n",
    "            register_c = 0\n",
    "            # This is a hack to create an array but it should be calculated from num_match_elements\n",
    "            # Use a persistent index because we can't append to the list\n",
    "            output_index = 0 \n",
    "            for i in range(num_match_elements):\n",
    "                output[i] = -1\n",
    "            # Program address\n",
    "            address_ptr = 0\n",
    "\n",
    "            while (address_ptr < len(program)):\n",
    "                # numba says these are int64\n",
    "                operator = program[address_ptr]\n",
    "                operand = program[address_ptr+1]\n",
    "                next_address_ptr = address_ptr+2\n",
    "                match (operator):\n",
    "                    case 0: # adv division register_a ~/ 2^comboOperand\n",
    "                        register_a = register_a // 2 ** resolve_operand(register_a, register_b, register_c, operand)\n",
    "                    case 1: # bxl bitwise XOR (registerB , operand)\n",
    "                        register_b = register_b ^ operand\n",
    "                    case 2: # bst operand modulo 8\n",
    "                        register_b = resolve_operand(register_a, register_b, register_c, operand) % 8\n",
    "                    case 3: # jnz jump not zero\n",
    "                        if (register_a != 0):\n",
    "                            next_address_ptr =  operand\n",
    "                    case 4: #bxc bitwise xor reg b, reg c\n",
    "                        register_b = register_b ^ register_c\n",
    "                    case 5: # out % modulo 8\n",
    "                        value = resolve_operand(register_a, register_b, register_c, operand) %8\n",
    "                        output[output_index]= value\n",
    "                        output_index += 1\n",
    "                    case 6: # BDV integer division on A, stored in B\n",
    "                        divisor = 2 ** resolve_operand(register_a, register_b, register_c, operand)\n",
    "                        register_b = register_a // divisor\n",
    "                    case 7: # CDV\n",
    "                        divisor = 2 ** resolve_operand(register_a, register_b, register_c, operand)\n",
    "                        register_c = register_a // divisor\n",
    "                    case _:\n",
    "                        #print('oh no')\n",
    "                        result = -1   \n",
    "                address_ptr = next_address_ptr\n",
    "                # This exists so I can experiment with different lengths while experimenting\n",
    "                if (match_sub_list(program,num_match_elements,output)):\n",
    "                    return_array[global_thread_id]= global_cycle_id\n",
    "                    return\n",
    "            # return_array[global_thread_id]= output_index\n",
    "            # return\n",
    "    # no matches if we get here\n",
    "    return_array[global_thread_id]=-1\n",
    "    return\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "# pass everything in so we don't get a capture error\n",
    "# implements the value/register value rules for an operand value\n",
    "def resolve_operand(register_a, register_b, register_c,operand):\n",
    "    if (operand < 4):\n",
    "        return operand\n",
    "    elif (operand ==4):\n",
    "        return register_a\n",
    "    elif (operand ==5):\n",
    "        return register_b\n",
    "    elif (operand ==6):\n",
    "        return register_c\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "# match some subset of the values in two lists\n",
    "def match_sub_list(program, num_match_elements, output_array) -> bool:\n",
    "    for i in range(len(program)):\n",
    "        if (i >= num_match_elements):\n",
    "            return True\n",
    "        if (program[i] != output_array[i]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db8c94-2385-4f8a-b631-f4051b7218a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# num_blocks_per_grid number of blocks in a grid (SMUs in GPUs or cores in CPUs)\n",
    "# num_threads_per_block = number of threads per core or number of threads that can run in an SMU\n",
    "#\n",
    "# thread id is the id within a block\n",
    "# block id is the id within a grid\n",
    "# block width is the number of threads per block\n",
    "# blocks per grid == SMUs * some occupancy factor\n",
    "# threads per block is some multiple of cores per SMU - they share memory\n",
    "\n",
    "# Titan RTX has 72 SMs with 64 processors -> 4608 cores\n",
    "num_blocks_per_grid = (72*1)\n",
    "num_threads_per_block = (64*2)\n",
    "# RTX 3060 TI has 38 SMs with 128 processors each -> 4864 cores\n",
    "# Used an occupancy calculator https://xmartlabs.github.io/cuda-calculator/ \n",
    "# It gave us 38*8\n",
    "num_blocks_per_grid = (38*4)\n",
    "num_threads_per_block = (1)\n",
    "\n",
    "program = np.array([2,4,1,3,7,5,4,7,0,3,1,5,5,5,3,0])\n",
    "total_num_threads = (num_blocks_per_grid * num_threads_per_block)\n",
    "return_array = np.zeros(total_num_threads, dtype=int)\n",
    "print(f\"Created return array sized {total_num_threads} for (b/g:{num_blocks_per_grid} x t/b:{num_threads_per_block})\")\n",
    "# print the entire results array in octal - useful while debugging\n",
    "np.set_printoptions(formatter={'int':oct},threshold=sys.maxsize)\n",
    "\n",
    "# copy the program and the results array to the GPU\n",
    "g_program = cuda.to_device(program)\n",
    "g_return_array = cuda.to_device(return_array)\n",
    "loop_em[num_blocks_per_grid,num_threads_per_block](        \n",
    "        g_program,\n",
    "        7,\n",
    "        g_return_array,\n",
    "        int(0o1000000000000000),\n",
    "        int(0o7777777777777777),\n",
    "       )\n",
    "return_array = g_return_array.copy_to_host()\n",
    "print(return_array)\n",
    "\n",
    "# 16 digits octal\n",
    "# 16th digit must be 1 otherwise the return is shorter than the program\n",
    "\n",
    "# 3060 ti\n",
    "# 05:49 with (38*8 x 128*2) matching 6 digits\n",
    "# 01:05 with (38*8 x 1    ) matching 6 digits\n",
    "# 03:09 with (38*4 x 128*2) matching 6 digits \n",
    "# 00:59 with (38*4 x 1    ) matching 6 digits\n",
    "# 01:42 with (38*2 x 128*2) matching 6 digits \n",
    "# 00:59 with (38*2 x 1    ) matching 6 digits\n",
    "# 01:18 with (38*1 x 128*2) matching 6 digits\n",
    "# 01:10 with (38*1 x 128*1) matching 6 digits alt run\n",
    "# 01:12 with (38*1 x 128*1) matching 6 digits\n",
    "# 00:59 with (38*1 x 1    ) matching 6 digits\n",
    "# 00:31 with ( 1   x 128*2) matching 6\n",
    "# 00:29 with ( 1   x 128*1) matching 6\n",
    "# 00:12 with ( 1   x 1    ) matching 6\n",
    "#\n",
    "# 17:36 with (38*4 x 1    ) matching 7 digits\n",
    "# 21:06 with (38*1 x 128*1) matching 7 digits\n",
    "# 17:30 with (38*1 x 1    ) matching 7 digits\n",
    "# 13:51 with ( 1   x 128*1) matching 7 digits\n",
    "# 14:08 with ( 1   x 32   ) matching 7 digits\n",
    "# 06:39 with ( 1   x 1    ) matching 7 digits\n",
    "#\n",
    "# 67:39 with ( 1   x 1    ) matching 8 digits\n",
    "#\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
